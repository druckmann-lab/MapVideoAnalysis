import torch
import torch.nn as nn
import numpy as np

class PreConvBlock(nn.Module):
    """
    PreConvBlock is a preprocessing convolutional block that applies a convolutional layer,
    followed by an optional batch normalization, ReLU activation, and max pooling.
    It is typically used at the beginning of a neural network to process input data.
    Args:
        in_channels (int): Number of input channels.
        out_channels (int): Number of output channels.
        kernel (int): Size of the convolutional kernel.
        stride (int): Stride for the convolutional layers.
        pool_size (int): Size of the max pooling layer.
        use_batch_norm (bool): Whether to use batch normalization.
    """
    def __init__(self, 
                 in_channels = 1, 
                 out_channels = 16, 
                 kernel = 3, 
                 stride=1, 
                 pool_size = 2,
                 use_batch_norm = False):
        """
        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            stride (int): Stride for the convolutional layers.
            kernel (int): Size of the convolutional kernel.
            pool_size (int): Size of the max pooling layer.
            use_batch_norm (bool): Whether to use batch normalization.
        """
        super(PreConvBlock, self).__init__()
        
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels, out_channels, 
                      kernel_size=kernel, stride=stride, 
                      padding=(kernel - stride) // 2, 
                      bias= not use_batch_norm),
        ])
        if use_batch_norm:
            self.layers.append(nn.BatchNorm2d(out_channels))
        
        self.layers.append(nn.ReLU(inplace=True))
        self.layers.append(nn.MaxPool2d(kernel_size=pool_size))
    
    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x
    
class ResidualBlock(nn.Module):
    """
    ResidualBlock is a building block for residual networks (ResNets).
    It consists of multiple convolutional layers with optional batch normalization and ReLU activation.
    Args:
        n_channels (int): Number of input and output channels.
        kernel (int): Size of the convolutional kernel.
        stride (int): Stride for the convolutional layers.
        use_batch_norm (bool): Whether to use batch normalization.
        downsample (nn.Module, optional): Downsampling layer if input and output dimensions differ.
        pool_size (int, optional): Size of the max pooling layer after the residual connection.
        n_layers (int): Number of convolutional layers in the block.
    """
    def __init__(self, 
                 n_channels = 16, 
                 kernel = 3, 
                 stride = 1, 
                 use_batch_norm = False, 
                 downsample=None,
                 pool_size = 4,
                 n_layers = 3):
        """
        Args:
            in_channels (int): Number of input channels.
            out_channels (int): Number of output channels.
            stride (int): Stride for the convolutional layers.
            downsample (nn.Module, optional): Downsampling layer if input and output dimensions differ.
            pool_size (int, optional): Size of the max pooling layer after the residual connection.
            n_layers (int): Number of convolutional layers in the block.
            kernel (int): Size of the convolutional kernel.
            use_batch_norm (bool): Whether to use batch normalization.
        """
        super(ResidualBlock, self).__init__()
        
        self.layers = nn.ModuleList()
        for i_layer in range(n_layers):
            self.layers.append(nn.Conv2d(n_channels, n_channels, 
                                          kernel_size=kernel, stride=stride, 
                                          padding=(kernel - stride) // 2, 
                                          bias= not use_batch_norm))
            if use_batch_norm:
                self.layers.append(nn.BatchNorm2d(n_channels))
            if i_layer!=(n_layers - 1):
                self.layers.append(nn.ReLU(inplace=True))
        
        self.downsample = downsample  # Optional downsampling layer
        self.post_residual_layers = nn.ModuleList([nn.ReLU(inplace=True)])
        if pool_size is not None:
            self.post_residual_layers.append(nn.MaxPool2d(kernel_size=pool_size))
    
    def forward(self, x):
        identity = x

        # Pass through the layers in the ModuleList
        for layer in self.layers:
            x = layer(x)

        # Apply downsampling to the identity if necessary
        if self.downsample:
            identity = self.downsample(identity)

        # Add the residual connection
        x += identity
        for layer in self.post_residual_layers:
            x = layer(x)

        return x
    
class Encoder(nn.Module):
    """
    The Encoder part of the autoencoder architecture.

    It starts with a preprocessing convolutional block, followed by a series of residual blocks and 
    ends with two linear layers.

    Args:
        configs (dict): Configuration dictionary containing parameters for the encoder.
            - in_channels_%d (list): List of input channels for each block.
            - out_channels_%d (list): List of output channels for each block.
            - kernel_preconv (int): Size of the convolutional kernel for the preprocessing block.
            - stride_preconv (int): Stride for the convolutional layers in the preprocessing block.
            - pool_size_preconv (int): Size of the max pooling layer in the preprocessing block.
            - use_batch_norm_preconv (bool): Whether to use batch normalization in the preprocessing block.
            - kernel_residual (int): Size of the convolutional kernel for the residual blocks.
            - stride_residual (int): Stride for the convolutional layers in the residual blocks.
            - use_batch_norm_residual (bool): Whether to use batch normalization in the residual blocks.
            - pool_size_residual_%d (list): List of pool sizes for each residual block.
            - n_layers_residual (int): Number of convolutional layers in each residual block.
            - out_conv (int): Output size after the convolutional layers.
            - out_linear (int): Output size after the linear layers.
            - embed_size (int): Size of the embedding vector.
            - num_blocks (int): Number of blocks in the encoder.
            - use_batch_norm_linear (bool): Whether to use batch normalization in the linear layers.
    """

    def __init__(self, 
                 configs):
        """
        Args:
            configs (dict): Configuration dictionary containing parameters for the encoder.
        """
        super(Encoder, self).__init__()
        
        self.configs = configs

        self.residual_layers = nn.ModuleList()

        for i_blocks in range(configs['num_blocks']):
            self.residual_layers.append(PreConvBlock(configs['in_channels_%d'%i_blocks],
                                            configs['out_channels_%d'%i_blocks],
                                            configs['kernel_preconv'],
                                            configs['stride_preconv'],
                                            configs['pool_size_preconv_%d'%i_blocks],
                                            configs['use_batch_norm_preconv']))
            self.residual_layers.append(ResidualBlock(configs['out_channels_%d'%i_blocks],
                                             configs['kernel_residual'],
                                             configs['stride_residual'],
                                             configs['use_batch_norm_residual'],
                                             pool_size=configs['pool_size_residual_%d'%i_blocks],
                                             n_layers=configs['n_layers_residual']))
            
        self.linear_layers = nn.ModuleList([nn.Linear(configs['out_conv'], configs['out_linear'], bias = not configs['use_batch_norm_linear']),])
        if configs['use_batch_norm_linear']:
            self.linear_layers.append(nn.BatchNorm1d(configs['out_linear']))
        self.linear_layers.append(nn.ReLU(inplace=True))
        self.linear_layers.append(nn.Linear(configs['out_linear'], configs['embed_size']))

    def forward(self, x):
        bs, seq_length, c, h ,w = x.size()
        
        x = x.view(bs*seq_length, c, h, w)
        for layer in self.residual_layers:
            x = layer(x)
        
        x = x.view(bs*seq_length, -1)
        for layer in self.linear_layers:
            x = layer(x)
        
        x = x.view(bs, seq_length, -1)
        return x
    
class SingleSessionDecoder(nn.Module):
    """
    The decoder part of the autoencoder that can be used for single session decoding.

    It is a single linear layer that maps the embedding latents into pixel space.
    Args:
        configs (dict): Configuration dictionary containing parameters for the decoder.
            - embed_size (int): Size of the embedding vector.
            - image_height (int): Height of the output image.
            - image_width (int): Width of the output image.
    """
    def __init__(self, configs):
        super(SingleSessionDecoder, self).__init__()
        self.configs = configs
        self.linear_layer = nn.Linear(configs['embed_size'], configs['image_height']*configs['image_width'])

    def forward(self, x):
        bs, seq_length, _ = x.size()
        x = self.linear_layer(x)
        x = x.view(bs, seq_length, 1, self.configs['image_height'], self.configs['image_width'])
        return x
    
class AutoEncoder(nn.Module):
    """
    AutoEncoder for mouse behavior to extract embedding latents from videos.
    It consists of a convolutional encoder followed by a linear decoder.

    Uses SingleSessionDecoder, latent sharing is not implemented here.
    Args: 
        configs (dict): Configuration dictionary containing parameters for autoencoder.
    """
    def __init__(self, configs):
        super(AutoEncoder, self).__init__()
        self.configs = configs
        self.encoder = Encoder(configs)
        self.decoder = SingleSessionDecoder(configs)
    
    def forward(self, x):
        z = self.encoder(x)
        x = self.decoder(z)
        return x, z