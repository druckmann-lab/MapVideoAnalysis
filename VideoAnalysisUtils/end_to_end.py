import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.transforms.functional as F
import numpy as np
from PIL import Image
import os

def pile_array_lightweight(data, number_of_neighbors=5, offset=0):
    """
    Piles up neighboring frames into a single tensor.

    Args:
        data (torch.Tensor): Input tensor of shape (batch, seq_len, features).
        number_of_neighbors (int): Number of neighboring frames to include.
        offset (int): Offset for the first neighbor frame.

    Returns:
        torch.Tensor: Piled tensor of shape (batch, new_seq_len, features*number_of_neighbors).
    """
    batch_size, seq_len, n_features = data.shape
    new_seq_len = seq_len - number_of_neighbors + 1
    if new_seq_len <= 0:
        raise ValueError("Invalid sequence length after piling.")

    # Create an empty tensor for the piled output
    piled = torch.stack([data[:, i:i + new_seq_len] for i in range(number_of_neighbors)], dim=3)
    piled = piled.reshape(batch_size, new_seq_len, -1)
    if offset > 0:
        # Shift the piled tensor to account for the offset
        piled = torch.roll(piled, shifts=offset, dims=1)

    return piled

class ResidualBlock2D(nn.Module):
    """
    2D residual block that is used in end-to-end models.

    Args:
        in_channels (int):  Number of input channels.
        out_channels (int): Number of output channels.
        kernel_preconv (int): Kernel size for the initial projection conv.
        kernel_residual (int): Kernel size for the residual convs.
        num_layers (int): Number of conv layers in the residual path.
        use_batch_norm (bool): Whether to use BatchNorm2d after each conv.
    """
    def __init__(self,
                 in_channels: int,
                 out_channels: int,
                 kernel_preconv: int = 1,
                 kernel_residual: int = 3,
                 num_layers: int = 3,
                 use_batch_norm: bool = False):
        """
        A 2D residual block with optional channel projection.

        Args:
            in_channels (int):  Number of input channels.
            out_channels (int): Number of output channels.
            kernel_preconv (int): Kernel size for the initial projection conv.
            kernel_residual (int): Kernel size for the residual convs.
            num_layers (int): Number of conv layers in the residual path.
            use_batch_norm (bool): Whether to use BatchNorm2d after each conv.
        """
        super().__init__()
        self.use_batch_norm = use_batch_norm

        # Projection if channel dims change
        if in_channels != out_channels:
            proj_layers = [
                nn.Conv2d(in_channels, out_channels,
                          kernel_size=kernel_preconv,
                          stride=1,
                          padding=(kernel_preconv - 1) // 2,
                          bias=not use_batch_norm)
            ]
            if use_batch_norm:
                proj_layers.append(nn.BatchNorm2d(out_channels))
            proj_layers.append(nn.ReLU(inplace=True))
            self.projection = nn.Sequential(*proj_layers)
        else:
            self.projection = None

        # Residual path
        res_layers = []
        for i in range(num_layers):
            res_layers.append(
                nn.Conv2d(out_channels, out_channels,
                          kernel_size=kernel_residual,
                          stride=1,
                          padding=(kernel_residual - 1) // 2,
                          bias=not use_batch_norm)
            )
            if use_batch_norm:
                res_layers.append(nn.BatchNorm2d(out_channels))
            if i < num_layers - 1:
                res_layers.append(nn.ReLU(inplace=True))
        self.residual = nn.Sequential(*res_layers)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        if self.projection is not None:
            proj = self.projection(x)
            res_in = proj
        else:
            proj = x
            res_in = x
        out = self.residual(res_in)
        out += proj
        return out


class Frame2FrEncoder(nn.Module):
    """
    End-to-end model to predict firing rates from frames. 

    The model consists of multiple Residual blocks followed by a linear layer.
    Args:
        configs (dict): Model configuration dictionary. Required keys:
            - in_channels (int): Number of input channels.
            - cnn_channels (list of int): List of output channels for each CNN layer.
            - kernel_preconv (int): Kernel size for the initial projection conv.
            - kernel_residual (int): Kernel size for the residual convs.
            - residual_depth (int): Number of conv layers in the residual path.
            - pool_size (int): Pooling size after each CNN block.
            - use_batch_norm (bool): Whether to use BatchNorm2d after each conv.
            - neighbor_frames (int): Number of neighboring frames to stack.
            - num_emit (int): Number of output neurons to predict firing rates for.
            - image_shape (tuple of int): Shape of input images as (height, width).

    """
    def __init__(self, configs: dict):
        """
        Args:
            configs (dict): Model configuration dictionary. Required keys:
                - in_channels (int)
                - cnn_channels (list of int)
                - kernel_preconv (int)
                - kernel_residual (int)
                - residual_depth (int)
                - pool_size (int)
                - use_batch_norm (bool)
                - neighbor_frames (int)
                - num_emit (int)
                - image_shape (tuple of int): (height, width)
        """
        super().__init__()
        self.configs = configs

        in_ch        = configs['in_channels']
        channels     = configs['cnn_channels']
        k_pre        = configs['kernel_preconv']
        k_res        = configs['kernel_residual']
        depth        = configs['residual_depth']
        pool_size    = configs['pool_size']
        use_bn       = configs['use_batch_norm']
        nbr_frames   = configs['neighbor_frames']
        num_emit     = configs['num_emit']
        img_h, img_w = configs['image_shape']

        # Build CNN feature extractor
        cnn_layers = []
        prev_ch = in_ch
        for out_ch in channels:
            # 1) the ResNet block
            cnn_layers.append(
                ResidualBlock2D(
                    in_channels=prev_ch,
                    out_channels=out_ch,
                    kernel_preconv=k_pre,
                    kernel_residual=k_res,
                    num_layers=depth,
                    use_batch_norm=use_bn
                )
            )
            # 2) then pool
            cnn_layers.append(nn.MaxPool2d(kernel_size=pool_size))
            # 3) then an explicit ReLU
            cnn_layers.append(nn.ReLU(inplace=True))
            prev_ch = out_ch
        self.cnn = nn.Sequential(*cnn_layers)

        # Compute flattened feature size after pooling and stacking
        total_down = pool_size ** len(channels)
        conv_h = img_h // total_down
        conv_w = img_w // total_down
        flat_size = conv_h * conv_w * channels[-1] * nbr_frames

        self.fc = nn.Linear(flat_size, num_emit)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x (torch.Tensor): Input of shape (batch, seq_len, C, H, W)
        Returns:
            torch.Tensor: Output of shape (batch, new_seq_len, num_emit)
        """
        bs, seq_len, C, H, W = x.shape
        # merge batch & seq dims for CNN
        x = x.view(bs * seq_len, C, H, W)
        x = self.cnn(x)
        # restore dims
        x = x.view(bs, seq_len, -1)
        # stack neighbor frames
        x = pile_array_lightweight(x, offset=0, number_of_neighbors=self.configs['neighbor_frames'])
        bs2, seq2, feat = x.shape

        x = x.view(bs2 * seq2, feat)

        # final linear layer
        x = self.fc(x)
        return x.view(bs, seq2, self.configs['num_emit'])

class CustomCropResize:
    """
    Custom transform to crop and resize images for neural network training.
    """
    def __init__(self, target_shape=(120, 112, 1), crop_info={'h_coord': 26}):
        """
        Args:
            target_shape: (height, width, channels)
            crop_info: dict with cropping parameters. For example, 
                       crop_info['h_coord'] determines the crop start as:
                       int(crop_info['h_coord'] / target_height * original_height + 0.5)
        """
        self.target_shape = target_shape
        self.crop_info = crop_info

    def __call__(self, img):
        # Ensure the image is in grayscale
        if img.mode != 'L':
            img = img.convert('L')
        # Get target height and width from target_shape
        h_target, w_target, _ = self.target_shape
        # Get original dimensions (PIL gives (width, height))
        original_width, original_height = img.size
        # Compute the vertical crop coordinate, analogous to your cv2 code
        crop_y = int(self.crop_info['h_coord'] / h_target * original_height + 0.5)
        # Crop the image: from crop_y to bottom, full width
        img = F.crop(img, crop_y, 0, original_height - crop_y, original_width)
        # Resize the image to the target dimensions.
        # F.resize expects size as (height, width).
        img = F.resize(img, (h_target, w_target))
        # Convert the image to a tensor (this scales pixel values to [0, 1])
        img = F.to_tensor(img)
        return img
    
class MiniFrameDataset(torch.utils.data.Dataset):
    """
    Dataset to load mini example dataset for training of end-to-end model.
    """
    def __init__(self, loaded_example_data, sequence_length=16, center_fr=True, center_frames=True):
        """
        Args:
            loaded_example_data (dict): Dictionary containing the example data.
            sequence_length (int): Length of each sequence.
            center_fr (bool): Whether to center the firing rates.
            center_frames (bool): Whether to center the frames.
        """
        self.frames = loaded_example_data['frames'] # shape: trial, timepoint, channel(1), height(120), width(112)
        self.FR_ephys = loaded_example_data['fr'] # shape: trial, timepoint, neurons(10)
        self.trials = loaded_example_data['trials']
        self.tt_ephys = loaded_example_data['tt']
        self.start_time = loaded_example_data['start_time']
        self.end_time = loaded_example_data['end_time']
        self.dt = loaded_example_data['dt']
        self.sequence_length = sequence_length
        self.center_fr = center_fr
        self.center_frames = center_frames
        self.n_sequences =  self.frames.shape[1] // self.sequence_length

        if self.center_fr:
            self.fr_mean, self.fr_std = self._get_mean_and_std(self.FR_ephys)
        else:
            self.fr_mean = 0.
            self.fr_std = 1.
        if self.center_frames:
            self.frame_mean, self.frame_std = self._get_mean_and_std(self.frames)
        else:
            self.frame_mean = 0.
            self.frame_std = 1.


    def __len__(self):
        return self.frames.shape[0] * self.n_sequences

    def _get_mean_and_std(self, array_3d):
        """Calculate mean and std for a 3D array."""
        mean = array_3d.mean(axis=(0, 1))
        std = array_3d.std(axis=(0, 1)) + np.finfo(float).eps
        return mean, std

    def __getitem__(self, idx):
        trial_ii = idx // self.n_sequences
        sequence_ii = idx % self.n_sequences

        fr_data = self.FR_ephys[trial_ii, sequence_ii * self.sequence_length:(sequence_ii + 1) * self.sequence_length]
        jpg_data = self.frames[trial_ii, sequence_ii * self.sequence_length:(sequence_ii + 1) * self.sequence_length]

        fr_data = (fr_data - self.fr_mean[None,:]) / self.fr_std[None,:]
        jpg_data = (jpg_data - self.frame_mean[None,...]) / self.frame_std[None,...]
            
        return jpg_data, fr_data, trial_ii, sequence_ii
    
###
# Utils and class for dataloading frames from raw folder structure.
###

def get_frames_between_limits(frame_inds, go_time, start_time = -2, end_time = 1.2, dt=0.0034):
    """
    Get frame indices that fall within the specified time limits.
    Args:
        frame_inds (np.ndarray): Array of frame indices.
        go_time (float): Time of the 'go' event.
        start_time (float): Start time for the frame selection, relative to 'go'.
        end_time (float): End time for the frame selection, relative to 'go'.
        dt (float): Time step between frames.
    Returns:
        np.ndarray: Indices of frames that fall within the specified time limits.
    """
    frame_times = frame_inds * dt - go_time
    starting_ind = np.where(frame_times <= start_time)[0][-1]
    ending_ind = starting_ind + int((end_time - start_time) / dt)
    frame_mask = (frame_inds >= starting_ind) * (frame_inds < ending_ind)
    return frame_inds[frame_mask]  # Return the indices of frames within the limits

def read_frame_from_folder(folder_name, frame_name):
    """
    Read a single frame from the specified folder and apply custom transformations.
    Args:
        folder_name (str): Path to the folder containing the frame.
        frame_name (str): Name of the frame file.
    Returns:
        img (tensor): Transformed image.
    """
    custom_transform = CustomCropResize(target_shape=(120, 112, 1), crop_info={'h_coord': 26})
    frame_path = os.path.join(folder_name, frame_name)
    img = Image.open(frame_path)
    img = custom_transform(img)  # Apply the custom transform
    return img

def load_frames_from_inds(folder_name, frame_inds):
    """
    Load frames from a folder based on specified indices.
    Args:
        folder_name (str): Path to the folder containing the frames.
        frame_inds (np.ndarray): Indices of frames to load.
    Returns:
        jpg_data (torch.Tensor): Stacked tensor of loaded frames.
    """
    frames = sorted([s for s in os.listdir(folder_name) if s.endswith('.jpg') and int(s.split('-')[-1].split('.')[0]) in frame_inds])
    jpg_data = []
    for frame_name in frames:
        img = read_frame_from_folder(folder_name, frame_name)
        jpg_data.append(img)
    jpg_data = torch.stack(jpg_data, dim=0)  # Stack into a tensor
    return jpg_data

class CustomDataset(torch.utils.data.Dataset):
    """
    Custom dataset for loading frames and firing rates from a folder structure.
    Args:
        frame_folder (str): Path to the folder containing frames.
        ephys_data (dict): Dictionary containing ephys data with keys 'bin_centers' and 'fr'.
        trials (list): List of trial names.
        go_times (list): List of go times for each trial.
        start_time (float): Start time for the sequence.
        end_time (float): End time for the sequence.
        dt (float): Time step between frames.
        sequence_length (int): Length of each sequence.
        frame_array_passed (bool): Whether the frame array is passed directly.
        center_fr (bool): Whether to center firing rates.
        center_frames (bool): Whether to center frames.
    """
    def __init__(self, frame_folder, ephys_data, trials, go_times, start_time = 0, end_time = 1., dt=0.0034, sequence_length = 64, frame_array_passed=False, center_fr=True, center_frames=False):

        self.frame_folder = frame_folder
        self.ephys_data = ephys_data
        self.go_times = go_times
        self.start_time = start_time
        self.end_time = end_time
        self.dt = dt
        self.sequence_length = sequence_length
        self.frame_array_passed = frame_array_passed
        self.center_fr = center_fr
        self.center_frames = center_frames
        if frame_array_passed:
            self.frames = frame_folder
            good_trial_mask = np.ones(len(trials), dtype=bool)  # All trials are considered good
        else:
            good_trial_mask = self.check_trials_intact(trials, go_times)
        self.trials = np.array(trials)[good_trial_mask]
        self.tt_ephys = ephys_data['bin_centers']
        self.FR_ephys = ephys_data['fr']
        time_mask = (self.tt_ephys >= self.start_time) & (self.tt_ephys <= self.end_time)
        self.this_tt = self.tt_ephys[time_mask]
        self.n_sequences = len(self.this_tt) // self.sequence_length
        self.get_fr_mean_and_std()
        self.get_frames_mean_and_std()

    def __len__(self):
        return self.n_sequences * len(self.trials)

    def check_single_trial_intact(self, trial_name, go_time):
        """Check if a single trial has all the frames intact."""
        
        frames = sorted([s for s in os.listdir(os.path.join(self.frame_folder, trial_name)) if s.endswith('.jpg')])
        frame_inds = np.array([int(f.split('-')[-1].split('.')[0]) for f in frames])
        frames_within_lims = get_frames_between_limits(frame_inds, go_time, start_time=self.start_time, end_time=self.end_time, dt=self.dt)
        expected_number_of_frames = int((self.end_time - self.start_time) / self.dt)
        
        if len(frames_within_lims) != expected_number_of_frames:
            print(f"Warning: Trial {trial_name} has {len(frames_within_lims)} frames, expected {expected_number_of_frames}.")
            return False
        if len(frames_within_lims) == 0:
            print(f"Warning: No frames found for trial {trial_name} within limits.")
            return False
        return True

    def check_trials_intact(self, trials, go_times):
        """Check if trials have all the frames intact."""
        trial_order = [int(trial_name.split('-')[-1])-1 for trial_name in trials]
        good_trial_mask = np.array([self.check_single_trial_intact(trial_name, go_time) for trial_name, go_time in zip(trials, go_times[trial_order])])
        return good_trial_mask
    
    def get_fr_mean_and_std(self):
        """
        Calculate mean and std for the firing rates.
        This is done only for the time window between start_time and end_time.
        """
        time_mask = (self.tt_ephys >= self.start_time) & (self.tt_ephys <= self.end_time)
        FR_bounds = self.FR_ephys[time_mask]
        fr_mean = FR_bounds.mean(axis=(0,1))
        fr_std = FR_bounds.std(axis=(0,1)) + np.finfo(float).eps  # Add small value to avoid division by zero
        self.fr_mean = fr_mean
        self.fr_std = fr_std
        return fr_mean, fr_std
    
    def get_frames_mean_and_std(self):
        """
        Calculate mean and std for the frames.
        
        Is only implemented if the frame array is passed, otherwise just returns 0,1.
        """
        if not self.frame_array_passed:
            self.frame_mean = 0.
            self.frame_std = 1.
            return 0., 1.
        
        else:
            frame_mean = self.frames.mean(axis = (0,1), keepdims=True)
            frame_std = self.frames.std(axis = (0,1), keepdims=True) + np.finfo(float).eps
            self.frame_mean = frame_mean
            self.frame_std = frame_std
            return frame_mean, frame_std

    def __getitem__(self, idx):
        trial_ii = idx // self.n_sequences
        sequence_ii = idx % self.n_sequences

        trial_name = self.trials[trial_ii]
        trial_idx = int(trial_name.split('-')[-1])
        if self.frame_array_passed:
            jpg_data = self.frames[trial_ii]
        else:
            frames = sorted([s for s in os.listdir(os.path.join(self.frame_folder, trial_name)) if s.endswith('.jpg')])
            frame_inds = np.array([int(f.split('-')[-1].split('.')[0]) for f in frames])
            frames_within_lims = get_frames_between_limits(frame_inds, self.go_times[trial_idx-1], start_time=self.start_time, end_time=self.end_time, dt=self.dt)
            jpg_data = load_frames_from_inds(os.path.join(self.frame_folder, trial_name), frames_within_lims)

        time_mask = (self.tt_ephys >= self.start_time) & (self.tt_ephys <= self.end_time)
        self.this_tt = self.tt_ephys[time_mask]
        this_FR = self.FR_ephys[time_mask][:,trial_idx]

        sequence_ids = np.arange(sequence_ii * self.sequence_length, (sequence_ii + 1) * self.sequence_length)
        this_FR = this_FR[sequence_ids]
        jpg_data = jpg_data[sequence_ids]

        if self.center_fr:
            this_FR = (this_FR - self.fr_mean) / self.fr_std
        if self.center_frames:
            jpg_data = (jpg_data - self.frame_mean) / self.frame_std
            
        return jpg_data, this_FR, trial_idx
