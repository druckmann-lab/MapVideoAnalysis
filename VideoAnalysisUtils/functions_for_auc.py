from sklearn.linear_model import RidgeClassifierCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score
import numpy as np


def random_split(trial_labels, cv = 20, random_seed = 42):
    """
    Randomly splits trials into training and testing sets for cross-validation.

    Parameters
    ----------
        trial_labels : array-like
            Labels for each trial, used to determine the split.
        cv : int, optional
            Number of cross-validation folds, by default 20.
        random_seed : int, optional
            Seed for random number generator, by default 42.
    Returns
    -------
        split_inds_dict : dict
            Dictionary containing training and testing indices for each fold.
    """
    np.random.seed(random_seed)
    n_trials = len(trial_labels)
    trial_inds = np.arange(n_trials)
    np.random.shuffle(trial_inds)
    cv_inds = np.array_split(trial_inds, cv)

    split_inds_dict = {'train':[], 'test':[]}
    for ii in range(cv):
        train_inds = np.concatenate([cv_inds[jj] for jj in range(cv) if jj != ii])
        test_inds = cv_inds[ii]
        split_inds_dict['train'].append(train_inds)
        split_inds_dict['test'].append(test_inds)

    return split_inds_dict


def calculate_single_neuron_auc_from_classification(
    fr_mean, 
    trial_labels, 
    cv = 20, 
    scale = False, 
    classifier = RidgeClassifierCV, 
    class_weight = 'balanced', 
    trial_min = 21, 
    train_min = 10,
    test_min = 1,
    lda_n_components = None,
    random_seed = 42,
    concat_folds = False,
    return_scores = False,
    keep_folds_separate = False):
    """
    Calculate AUC for a single neuron using classification.

    Parameters
    ----------
        fr_mean : array-like
            Mean firing rates for each trial, shape (n_trials, n_neurons).
        trial_labels : array-like
            Labels for each trial, shape (n_trials,).
        cv : int, optional
            Number of cross-validation folds, by default 20.
        scale : bool, optional
            Whether to scale the firing rates, by default False.
        classifier : class, optional
            Classifier to use for classification, by default RidgeClassifierCV.
        class_weight : str or dict, optional
            Class weights for the classifier, by default 'balanced'.
        trial_min : int, optional
            Minimum number of trials per class for a valid split, by default 21.
        train_min : int, optional
            Minimum number of training trials per class, by default 10.
        test_min : int, optional
            Minimum number of testing trials per class, by default 1.
        lda_n_components : int, optional
            Number of components for LDA, by default None (not used).
        random_seed : int, optional
            Seed for random number generator, by default 42.
        concat_folds : bool, optional
            Whether to concatenate results across folds, by default False.
        return_scores : bool, optional
            Whether to return the scores for each fold, by default False.
        keep_folds_separate : bool, optional
            Whether to keep the folds separate in the output, by default False.
    Returns
    -------
        auc_test : float or array
            AUC for the test set, averaged across folds if `concat_folds` is False.
        auc_train : float or array
            AUC for the training set, averaged across folds if `concat_folds` is False.
        test_scores : array, optional
            Decision function scores for the test set, returned if `return_scores` is True.
        train_scores : array, optional
            Decision function scores for the training set, returned if `return_scores` is True.
    """

    _seed = random_seed

    n_categories = len(np.unique(trial_labels))
    if n_categories != 2:
        raise ValueError('Only binary classification is supported for now. Number of categories: {}'.format(n_categories))

    if np.sum(trial_labels == np.unique(trial_labels)[0]) < trial_min or np.sum(trial_labels == np.unique(trial_labels)[1]) < trial_min:
        raise ValueError('At least one of the classes has fewer than {} trials'.format(trial_min))

    not_valid_split = True
    while not_valid_split:
        split_inds_dict = random_split(trial_labels, cv = cv, random_seed = _seed)
        not_valid_split = False
        for ii in range(cv):
            train_inds = split_inds_dict['train'][ii]
            test_inds = split_inds_dict['test'][ii]
            if np.sum(trial_labels[train_inds] == np.unique(trial_labels)[0]) < train_min or np.sum(trial_labels[train_inds] == np.unique(trial_labels)[1]) < train_min:
                not_valid_split = True
                _seed += 1
            if np.sum(trial_labels[test_inds] == np.unique(trial_labels)[0]) < test_min or np.sum(trial_labels[test_inds] == np.unique(trial_labels)[1]) < test_min:
                not_valid_split = True
                _seed += 1

    
    model = classifier(class_weight = class_weight)
    test_scores = []
    train_scores = []

    for ii in range(cv):
        train_inds = split_inds_dict['train'][ii]
        test_inds = split_inds_dict['test'][ii]

        if scale:
            scaler = StandardScaler()
            fr_train = scaler.fit_transform(fr_mean[train_inds])
            fr_test = scaler.transform(fr_mean[test_inds])
        else:
            fr_train = fr_mean[train_inds]
            fr_test = fr_mean[test_inds]

        model.fit(fr_train, trial_labels[train_inds])
        
        test_scores.append(model.decision_function(fr_test))
        train_scores.append(model.decision_function(fr_train))

    if concat_folds:
        test_scores = np.concatenate(test_scores)
        train_scores = np.concatenate(train_scores)
        test_labels = np.concatenate([trial_labels[split_inds_dict['test'][ii]] for ii in range(cv)])
        train_labels = np.concatenate([trial_labels[split_inds_dict['train'][ii]] for ii in range(cv)])

        auc_test = roc_auc_score(test_labels, test_scores)
        auc_train = roc_auc_score(train_labels, train_scores)

    else:
        if keep_folds_separate:
            auc_test = np.array([roc_auc_score(trial_labels[split_inds_dict['test'][ii]], test_scores[ii]) for ii in range(cv)])
            auc_train = np.array([roc_auc_score(trial_labels[split_inds_dict['train'][ii]], train_scores[ii]) for ii in range(cv)])
        else:
            auc_test = np.mean([roc_auc_score(trial_labels[split_inds_dict['test'][ii]], test_scores[ii]) for ii in range(cv)])
            auc_train = np.mean([roc_auc_score(trial_labels[split_inds_dict['train'][ii]], train_scores[ii]) for ii in range(cv)])
    
    if return_scores:
        return auc_test, auc_train, test_scores, train_scores
    else:
        return auc_test, auc_train
    
def calculate_multi_neuron_auc(fr_mean, group1_labels, group2_labels, cv = 20, concat_folds = False, keep_folds_separate = False):
    """
    Calculate AUC for multiple neurons by performing classification on firing rates.

    Parameters
    ----------
        fr_mean : array-like
            Mean firing rates for each trial, shape (n_trials, n_neurons).
        group1_labels : array-like
            Boolean array or indices for the first group of trials.
        group2_labels : array-like
            Boolean array or indices for the second group of trials.
        cv : int, optional
            Number of cross-validation folds, by default 20.
        concat_folds : bool, optional
            Whether to concatenate results across folds, by default False.
        keep_folds_separate : bool, optional
            Whether to keep the folds separate in the output, by default False.
    Returns
    -------
        auc_array_test : array
            AUC for the test set for each neuron, shape (n_neurons,) or (n_neurons, cv) if `keep_folds_separate` is True.
        auc_array_train : array
            AUC for the training set for each neuron, shape (n_neurons,) or (n_neurons, cv) if `keep_folds_separate` is True.
    """

    n_neurons = fr_mean.shape[1]
    fr1 = fr_mean[group1_labels]
    fr2 = fr_mean[group2_labels]

    _fr = np.concatenate([fr1, fr2], axis = 0)
    _labels = np.concatenate([np.zeros(fr1.shape[0]), np.ones(fr2.shape[0])])

    if keep_folds_separate:
        auc_array_test = np.zeros((n_neurons, cv))
        auc_array_train = np.zeros((n_neurons, cv))
    else:
        auc_array_test = np.zeros(n_neurons)
        auc_array_train = np.zeros(n_neurons)

    for ii in range(n_neurons):
        auc_test, auc_train = calculate_single_neuron_auc_from_classification(_fr[:,ii].reshape(-1, 1), _labels, cv = cv, concat_folds = concat_folds, keep_folds_separate=keep_folds_separate)
        auc_array_test[ii] = auc_test
        auc_array_train[ii] = auc_train

    return auc_array_test, auc_array_train