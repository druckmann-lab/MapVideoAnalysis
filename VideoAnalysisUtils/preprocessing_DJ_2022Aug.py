"""
Preprocessing script from Yi for the raw .mat files of the MAP dataset
that were exported from DataJoint.
"""

import os, sys, time, pickle, argparse, math, glob
from collections import OrderedDict
import csv
from itertools import compress
import multiprocessing

import matplotlib
import matplotlib.pyplot as plt
import numpy as np
# import pandas as pd
import scipy.io as sio
from scipy.stats import sem
import VideoAnalysisUtils.preprocessing_utils as preprocessing_utils

"""
Important notes on reading MATLAB files using Python:
When we read an Nx1 MATLAB cell into a length-N Python list/array/anything iterable (and potentially do something before storing it into an iterable, e.g. if the cell contains indices, we might need to subtract 1 from each element since MATLAB counts from 1 and Python counts from 0), pay attention to the following three cases:
1. Empty 0x1 cell. Need to manually define what it becomes in Python.
2. 1x1 cell. Will be directly read as non-iterables such as str, int, float, etc. Need to manually convert it to an iterable with length 1.
3. Nx1 cell (N >= 2). Usually it's fine to read and directly store it in Python.

Comment: in case 2, when using isinstance(data, data_type), if data could be a number, consider the possibility of data being either float or int, i.e. use
    if isinstance(data, float) or isinstance(data, int):
        do something
"""

plt.rcParams['pdf.fonttype'] = 42
plt.rcParams['ps.fonttype'] = 42


def sliding_histogram(spikeTimes, begin_time, end_time, bin_width, stride, rate=True):
    '''
    Calculates the number of spikes for each unit in each sliding bin of width bin_width, strided by stride.
    begin_time and end_time are treated as the lower- and upper- bounds for bin centers.
    if rate=True, calculate firing rates instead of bin spikes.
    '''
    bin_begin_time = begin_time
    bin_end_time = end_time
    # This is to deal with cases where for e.g. (bin_end_time-bin_begin_time)/stride is actually 43 but evaluates to 42.999999... due to numerical errors in floating-point arithmetic.
    if np.allclose((bin_end_time-bin_begin_time)/stride, math.floor((bin_end_time-bin_begin_time)/stride)+1):
        n_bins =  math.floor((bin_end_time-bin_begin_time)/stride)+2
    else:
        n_bins = math.floor((bin_end_time-bin_begin_time)/stride)+1
    binCenters = bin_begin_time + np.arange(n_bins)*stride

    binIntervals = np.vstack((binCenters-bin_width/2, binCenters+bin_width/2)).T

    # Count number of spikes for each sliding bin
    binSpikes = np.asarray([[[np.sum(np.all([trial>=binInt[0], trial<binInt[1]], axis=0)) for binInt in binIntervals] for trial in unit]
        for unit in spikeTimes]).swapaxes(0,-1)

    if rate:
        return binCenters, binSpikes/float(bin_width) # get firing rates
    return binCenters, binSpikes


def process_one_sess(data_folder, file_path_list, bw, stride, begin_time=-3.0, end_time=3.5, save_folder=os.path.join(os.pardir, 'data_processed'), qc_mode='classifier'):
    """
    Process all probe files in one session.

    Args:
        data_folder (str): the folder containing all data including QC neuron ID files.
        file_path_list: list of full file paths of all probes in this session.
        bw (float): in sec.
        stride (float): in sec.
        qc_mode (str): 'classifier' or 'old'.
    """
    print(qc_mode)
    if qc_mode == 'classifier':
        qc_folder = os.path.join(data_folder, 'goodunits')
        print(qc_folder)
    elif qc_mode == 'old':
        qc_folder = os.path.join(data_folder, 'DJ_GoodUnitsIdx_14regions_old_qc')
    else:
        raise ValueError('Wrong qc_mode! Must be "classifier" or "old".')
    
    assert os.path.exists(qc_folder)

    spike_times_sess = []
    unit_info_sess = []
    unit_qc_sess_list = []
    ccf_coordinate_sess = []
    ccf_label_sess = []
    ccf_unit_id_sess = []

    # ----- 1. concatenate all neurons from all probes -----
    for i_file, file_path in enumerate(sorted(file_path_list)):
        filename = os.path.split(file_path)[-1]
        name_list = filename[:-4].split('_') # ['map-export, 'SC038', '20191119', '115109', 's4', 'p1']
        mouse = name_list[1] # 'SC038'
        date = name_list[2] # '20191119'
        number = name_list[3]
        session = int(name_list[4][1:]) # 4
        probe = int(name_list[5][1:]) # 1
        sess_name = '%s_%s_%d' % (mouse, date, session)
        sess_name_qc = '_'.join([mouse, date, number]) # 'SC038_20191119_115109'

        save_folder_sess = os.path.join(save_folder, 'stride%d_bw%d'%(stride*1000, bw*1000), sess_name)
        if not os.path.exists(save_folder_sess):
            os.makedirs(save_folder_sess)

        if os.path.exists(os.path.join(save_folder_sess, 'others_finished.txt')):
            print('%s has been processed. Skip.' % sess_name)
            return # HERE

        print('Reading probe file', probe)
        # sess_file = utils.loadmat(os.path.join(file_path))
        sess_file = preprocessing_utils.loadmat(file_path)

        # --- 1.1 behaviors and tasks ---
        auto_learn_trials = sess_file['behavior_auto_learn'] # ndarray of str, '1' is autolearn, '4' is non-autolearn.
        auto_learn_trials = np.asarray(auto_learn_trials, dtype=int)
        early_lick_trials = sess_file['behavior_early_report'] # ndarray, (n_trials,), int64
        auto_water_trials = sess_file['behavior_is_auto_water'] # ndarray, (n_trials,), int64
        free_water_trials = sess_file['behavior_is_free_water'] # ndarray, (n_trials,), int64
        lick_directions = sess_file['behavior_lick_directions'] # ndarray, (n_trials). Each element is a list of 0 (lick left) and 1 (lick right)
        lick_times = sess_file['behavior_lick_times'] # lick_times[trial_id] is ndarray, each element is a list of lick times (need to subtract go cue time from it)
        correctness = sess_file['behavior_report'] # ndarray, (n_trials,), int64. 1: correct or free water, 0: error, -1: no response
        gocue_time = sess_file['task_cue_time'] # ndarray, (2,n_trials). First row is go cue time, second row is duration of response period
        delay_time = sess_file['task_delay_time'] # ndarray, (2,n_trials). First row is delay time, second row is duration of delay period
        sample_time = sess_file['task_sample_time'] # ndarray, (2,n_trials). First row is sample time, second row is duration of sample period
        delay_period = delay_time[1,:] # only need duration of delay period
        sample_period = sample_time[1,:] # only need duration of sample period
        stimulation = sess_file['task_stimulation'] # ndarray, (n_trials,4). The 4 columns are [laser_power, stim_type('1', '2', or '6', with 1,2,6 being left/right/both ALM perturbation), laser_on_time, laser_off_time] (with reference to trial start time - need to subtract go cue time from it).
        trial_type = sess_file['task_trial_type'] # ndarray, (1,n_trials), 'l' or 'r'

        # align stimulation time and lick times to gocue_time=0 #
        lick_time_cal = []
        for i_trial, lick_times_trial in enumerate(lick_times):
            gocue_time_trial = gocue_time[0, i_trial]
            lick_time_cal.append(lick_times_trial - gocue_time_trial)
            stimulation[i_trial, 2:] -= gocue_time_trial
        lick_times = lick_time_cal

        if i_file == 0:
            ### only save behavior and task info for the first probe file ###
            sess_dict = {}
            sess_dict['sess_name'] = sess_name
            sess_dict['auto_learn_trials'] = auto_learn_trials
            sess_dict['early_lick_trials'] = early_lick_trials # ndarray, (n_trials,), int64
            sess_dict['auto_water_trials'] = auto_water_trials # ndarray, (n_trials,), int64
            sess_dict['free_water_trials'] = free_water_trials # ndarray, (n_trials,), int64
            sess_dict['lick_directions'] = lick_directions # ndarray, (n_trials). Each element is a list of 0 (lick right) and 1 (lick left)
            sess_dict['lick_times'] = lick_time_cal # list with length n_trials, each element is 1D ndarray with all lick times in that trial
            sess_dict['gocue_time'] = gocue_time[0,:]
            sess_dict['correctness'] = correctness # ndarray, (n_trials,), int64. 1: correct or free water, 0: error, -1: no response
            sess_dict['delay_period'] = delay_period # (n_trials,) duration of delay period
            sess_dict['sample_period'] = sample_period # (n_trials,) duration of sample period
            sess_dict['stimulation'] = stimulation # ndarray, (n_trials,4). The 4 columns are [laser_power, stim_type('1', '2', or '6', with 1,2,6 being left/right/both ALM perturbation), laser_on_time, laser_off_time]. For no stim trials, we have [0, NaN, NaN, NaN]
            sess_dict['trial_type'] = 1*(trial_type=='l') # ndarray, (n_trials,). 1 is left, 0 is right 'r'
            # with open(os.path.join(save_folder_sess, 'behavior_task.pickle'), 'wb') as f:
            #     pickle.dump(sess_dict, f)

            ### Read the neuron IDs after QC for this session ###
            '''
            The index refers to within individual SESSION, resulting in summing all units from all penetrations in a given Session. For instance, if in Session 08/17/22, we have 3 penetrations: p1 has 10 units, p2 has 30 units, p3 has 5 units (before filtering) - overall there are 45 units. An index of '42' from this session means the 2nd (42-10-30) unit from p3.
            '''
            temp_list = list(glob.glob(os.path.join(qc_folder, '{}*.mat'.format(sess_name_qc))))
            assert len(temp_list) == 1
            filepath_qc = temp_list[0]

            qc_file = preprocessing_utils.loadmat(filepath_qc)
            # idx_qc_dict = qc_file['Idx'] # dict. Keys are 'region_qc' (e.g. 'ALM_qc'). Values are list of good neuron IDs (in MATLAB it starts from 1 but here we change it to start from 0).
            idx_qc_dict = {}# dict. Keys are 'region_qc' (e.g. 'ALM_qc'). Values are list of good neuron IDs (in MATLAB it starts from 1 but here we change it to start from 0).
            for area, id_list in qc_file['Idx'].items():
                if id_list == []:
                    idx_qc_dict[area] = []
                elif isinstance(id_list, int) or isinstance(id_list, float): # only one neuron
                    idx_qc_dict[area] = [id_list-1]
                else:
                    idx_qc_dict[area] = list(np.array(id_list) - 1)

            anno_qc_dict = {}
            for name, value in qc_file['AnnoName'].items():
                if isinstance(value, str):
                    anno_qc_dict[name] = [value]
                else:
                    anno_qc_dict[name] = value

        # --- 1.2 neural data ---
        # -- 1.2.1 spike times
        spike_times = sess_file['neuron_single_units'] # ndarray. spike_times[neuron_id][trial_id] is ndarray

        # -- 1.2.2 unit_info and QC stats
        unit_info = sess_file['neuron_unit_info'] # ndarray (n_units, 8)
        # neuron_unit_info: columns: unit_id, unit_quality, unit_x_in_um, depth_in_um, associated_electrode, shank, cell_type, recording_location. For putative type, we have: 'Pyr' - pyramidal neurons, 'FS' - fast spiking, 'not classified', 'all' - all types
        probe_no_col = probe * np.ones((unit_info.shape[0], 1)) # (n_units,1) original probe #
        unit_info = np.concatenate([unit_info, probe_no_col], axis=1) # (n_units, 9)
        unit_qc = sess_file['neuron_unit_quality_control']
        # dictionary. Needed keys: 'unit_amp', 'presence_ratio', 'amplitude_cutoff', 'isi_violation', 'unit_snr', 'avg_firing_rate', 'drift_metric'. Values are all list with length n_neurons.

        # -- 1.2.3 histology
        histology = sess_file['histology'] # dictionary, keys: 'unit', 'ccf_x', 'ccf_y', 'ccf_z', 'annotation'
        ccf_unit_id = np.array(histology['unit']) # ndarray, (n_neurons,)
        ccf_coordinate = np.stack([histology['ccf_x'], histology['ccf_y'], histology['ccf_z']]).swapaxes(0,1) # ndarray (n_neurons,3), xyz coordinates
        ccf_label = histology['annotation'] # list of np.str_
        ccf_label = [str(x).strip() for x in ccf_label] # list of string, with both whitespace at the beginning and end of each string removed

        # -- 1.2.4 get units that have both ephys and histology
        unit_comb = set(unit_info[:,0]) & set(ccf_unit_id) # get the id of units that have both ephys and histology
        mask_ephys = [True if x in unit_comb else False for x in unit_info[:,0]] # mask for all other data (ephys) except histology
        mask_histology =[True if x in unit_comb else False for x in ccf_unit_id] # mask for histology only

        # mask CCF info
        ccf_coordinate = ccf_coordinate[mask_histology,:] # ndarray (n_neurons,3), xyz coordinates
        ccf_label = list(compress(ccf_label, mask_histology)) # list of string, with both whitespace at the beginning and end of each string removed
        ccf_unit_id = ccf_unit_id[mask_histology]

        # mask ephys info
        unit_info = unit_info[mask_ephys,:] # (n_neurons, 9)
        spike_times = spike_times[mask_ephys]
        for name in unit_qc.keys():
            unit_qc[name] = list(np.array(unit_qc[name])[mask_ephys])

        # --- 1.3 add probe data to sess data ---
        spike_times_sess.append(spike_times) # ndarray. spike_times[neuron_id][trial_id] is ndarray
        unit_info_sess.append(unit_info) # (n_units, 9)
        unit_qc_sess_list.append(unit_qc) # dictionary. Needed keys: 'unit_amp', 'presence_ratio', 'amplitude_cutoff', 'isi_violation', 'unit_snr', 'avg_firing_rate', 'drift_metric'. Values are all list with length n_neurons.
        ccf_coordinate_sess.append(ccf_coordinate) # ndarray (n_neurons,3), xyz coordinates
        ccf_label_sess = ccf_label_sess + ccf_label # list
        ccf_unit_id_sess.append(ccf_unit_id) # ndarray, (n_neurons,)

    # ----- 2. combine data from all probes ---
    spike_times_sess = np.concatenate(spike_times_sess) # ndarray. spike_times[neuron_id][trial_id] is ndarray
    unit_info_sess = np.concatenate(unit_info_sess, axis=0) # (n_units, 9)

    # uncommented
    unit_qc_sess = {'unit_amp': [], 'presence_ratio': [], 'amplitude_cutoff': [], 'isi_violation': [], 'avg_firing_rate':[], 'drift_metric': []}
    for probe_qc_dict in unit_qc_sess_list:
        for qc_name in unit_qc_sess.keys():
            unit_qc_sess[qc_name] = unit_qc_sess[qc_name] + probe_qc_dict[qc_name]

    ccf_coordinate_sess = np.concatenate(ccf_coordinate_sess, axis=0) # ndarray (n_neurons,3), xyz coordinates
    ccf_unit_id_sess = np.concatenate(ccf_unit_id_sess) # ndarray, (n_neurons,)

    n_neurons = len(spike_times_sess)
    assert unit_info_sess.shape[0] == n_neurons
    assert len(unit_qc_sess['drift_metric']) == n_neurons
    assert ccf_coordinate_sess.shape[0] == n_neurons
    assert len(ccf_label_sess) == n_neurons
    assert len(ccf_unit_id_sess) == n_neurons
    # print('total neurons:', n_neurons)

    # ----- 3. Process neurons from areas of interest and perform QC filtering -----
    i_file = 1
    neuron_id_used = []
    neuron_id_all = np.arange(n_neurons) # neuron_id is continuous from 0. unit_id is the original id from DJ.

    for region in ['ALM', 'Medulla', 'Midbrain', 'Striatum', 'Thalamus', 'Pons', 'Cerebellum', 'Hypothalamus', 'Hippocampus', 'Orbital', 'OtherCortex', 'Olfactory', 'CorticalSubplate', 'Pallidum']:
        for side in ['left', 'right']:
            area = '_'.join([side, region])
            area_key = '{}_qc'.format(region)
            neuron_id_area = helper_get_neuron_id_area(idx_qc_dict[area_key], anno_qc_dict[area_key], ccf_coordinate_sess, ccf_label_sess, side, region)

            if len(neuron_id_area) == 0:
                continue

            print('Filtered neurons:', area, len(neuron_id_area))

            # record used neurons
            neuron_id_used = neuron_id_used + neuron_id_area

            save_flag = process_one_area(sess_name, area, neuron_id_area, i_file, bw, stride, begin_time, end_time, save_folder_sess, spike_times_sess, unit_info_sess, unit_qc_sess, ccf_coordinate_sess, ccf_label_sess, ccf_unit_id_sess, sess_dict)

            if save_flag:
                i_file += 1

    # ----- 4. Don't process the rest of the neurons -----
    with open(os.path.join(save_folder_sess, 'others_finished.txt'), 'w') as f:
        f.write('{}: finished.'.format(sess_name))

    '''
    neuron_id_others = set(neuron_id_all) - set(neuron_id_used)
    neuron_id_others = sorted(list(neuron_id_others))

    if len(neuron_id_others)==0:
        return

    print('Filtered neurons:', 'others', len(neuron_id_others))
    process_one_area(sess_name, 'others', neuron_id_others, i_file, bw, stride, begin_time, end_time, save_folder_sess, spike_times_sess, unit_info_sess, ccf_coordinate_sess, ccf_label_sess, ccf_unit_id_sess, sess_dict)
    '''


def process_one_area(sess_name, area, neuron_id_area, i_file, bw, stride, begin_time, end_time, save_folder_sess, spike_times_sess, unit_info_sess, unit_qc_sess, ccf_coordinate_sess, ccf_label_sess, ccf_unit_id_sess, sess_dict):
    """
    Get all neurons in an area, extract firing rates, and then save data.
    """
    if os.path.exists(os.path.join(save_folder_sess, '%d_%s.pickle' % (i_file, area))):
        return True

    # filter using the neuron mask
    n_units = unit_info_sess.shape[0]
    spike_times_area = spike_times_sess[neuron_id_area]
    unit_info_area = unit_info_sess[neuron_id_area,:] # (n_units, 9)

    # uncommented
    unit_qc_area = {}
    for qc_name in unit_qc_sess.keys():
        unit_qc_area[qc_name] = np.array(unit_qc_sess[qc_name])[neuron_id_area]
        assert len(unit_qc_sess[qc_name]) == n_units

    ccf_coordinate_area = ccf_coordinate_sess[neuron_id_area,:] # ndarray (n_neurons,3)
    ccf_label_area = list(np.array(ccf_label_sess)[neuron_id_area])
    ccf_unit_id_area = list(ccf_unit_id_sess[neuron_id_area])

    # truncate spike times into desired time window (already aligned to go cue in raw data)
    for i_cluster, st_cluster in enumerate(spike_times_area):
        for i_trial, st_trial in enumerate(st_cluster):
            # if spike_times[i_cluster][i_trial] != []:
            #     amin_all.append(np.amin(spike_times[i_cluster][i_trial]))
            #     amax_all.append(np.amax(spike_times[i_cluster][i_trial]))
            if isinstance(st_trial, float) or isinstance(st_trial, int):
                spike_times_area[i_cluster][i_trial] = np.array([st_trial])
            # temp = spike_times[i_cluster][i_trial].copy()
            temp = spike_times_area[i_cluster][i_trial] # in Susu's data, spike_times are relative to go cue time
            spike_times_area[i_cluster][i_trial] = spike_times_area[i_cluster][i_trial][(temp>=begin_time)*(temp<=end_time)]

    # bin spike times into firing rates
    bin_centers, fr = sliding_histogram(spike_times_area, begin_time, end_time, bin_width=bw, stride=stride, rate=True) # fr: (n_bins, n_trials, n_neurons)

    if os.path.exists(os.path.join(save_folder_sess, '%d_%s.pickle' % (i_file, area))):
        with open(os.path.join(save_folder_sess, '%d_%s.pickle' % (i_file, area)), 'rb') as f: # HERE
            area_dict_exist = pickle.load(f)
            fr = area_dict_exist['fr']
            spike_times_area = area_dict_exist['spike_times']
            bin_centers = area_dict_exist['bin_centers']

    # organize data into dict
    area_dict = {}
    area_dict['sess_name'] = sess_name # str
    area_dict['area'] = area # str

    area_dict['bin_centers'] = bin_centers
    area_dict['fr'] = fr # (n_bins, n_trials, n_neurons)
    area_dict['spike_times'] = spike_times_area # ndarray. spike_times[neuron_id][trial_id] is ndarray with all spike times of this neuron in this trial. (go cue time is 0.)
    area_dict['neuron_info'] = unit_info_area # ndarray (n_units, 9)
    #  columns: unit_id, unit_quality, unit_x_in_um, depth_in_um, associated_electrode, shank, cell_type, recording_location, probe #. For putative type, we have: 'Pyr' - pyramidal neurons, 'FS' - fast spiking, 'not classified', 'all' - all types
    area_dict['unit_qc'] = unit_qc_area # dict. Keys are 'unit_amp', 'presence_ratio', 'amplitude_cutoff', 'isi_violation',, 'avg_firing_rate', 'drift_metric'. Values are all list with length n_neurons.
    area_dict['ccf_coordinate'] = ccf_coordinate_area # ndarray (n_neurons,3), xyz coordinates
    area_dict['ccf_label'] = ccf_label_area # list of string, with whitespace at the beginning and end of each string removed
    area_dict['ccf_unit_id'] = ccf_unit_id_area # ndarray (n_neurons,), id of units with histology

    for name, info in sess_dict.items():
        area_dict[name] = info

    # save data
    with open(os.path.join(save_folder_sess, '%d_%s.pickle' % (i_file, area)), 'wb') as f:
        pickle.dump(area_dict, f)

    return True



def helper_get_neuron_id_area(neuron_idx_list, anno_qc_list, ccf_coor, ccf_label, side, region):
    """
    Filter the neurons in data_sess so that only neurons in side_region is left.

    Args:
        neuron_idx_list (list): list of good neuron IDs.
        anno_qc_list (list): list of CCF annotations of good neurons.
        ccf_coor (ndarray): ndarray (n_neurons,3), xyz coordinates.
        ccf_label (list): list of string (CCF annotations), with length n_neurons.
        side (str): 'left' or 'right'.
        region (str): 'ALM', 'Medulla', 'Midbrain', 'Striatum', 'Thalamus', 'BLA', or 'ECT'.

    Returns:
        neuron_id_area (list): id of neurons within side_region.
    """
    if len(neuron_idx_list) == 0:
        return []

    n_neurons = ccf_coor.shape[0]
    assert len(ccf_label) == n_neurons
    neurons_all = np.arange(n_neurons)

    # filter neurons using side
    ML_mid_coor = 5700
    if side == 'left':
        mask_side = ccf_coor[:,0] >= ML_mid_coor
    elif side == 'right':
        mask_side = ccf_coor[:,0] < ML_mid_coor
    neuron_id_side = neurons_all[mask_side]

    if len(neuron_id_side) == 0:
        return []

    # combine side filter with qc filter
    neuron_id_area = sorted(list(set(neuron_id_side) & set(neuron_idx_list)))
    neuron_id_area = [idx for idx in neuron_id_area if ccf_label[idx] != []]
    if len(neuron_id_area) == 0:
        return []
    ccf_label_list = list(np.array(ccf_label)[neuron_id_area])
    ccf_label_joined = ''.join(ccf_label_list)

    idx_side = [i for i, idx in enumerate(neuron_idx_list) if idx in neuron_id_area]
    anno_qc_list = list(np.array(anno_qc_list)[idx_side])
    anno_qc_joined = ''.join(anno_qc_list)

    # temp = list(np.array(ccf_label)[neuron_id_area])
    # print(len(temp), len(anno_qc_list))
    # num = np.amin([len(temp), len(anno_qc_list)])
    # for ii in range(num):
        # if temp[ii] != anno_qc_list[ii]:
        # print(ii, temp[ii], anno_qc_list[ii])
    assert ccf_label_joined == anno_qc_joined

    '''
    # filter neurons using region
    if region == 'ALM':
        res = 100 # resolution: 100 um
        # ALM_voxels = np.load(open('ALM_voxels_raw.npy', 'rb')) # (n_voxels,3), xyz of ALM
        ALM_voxels = np.load(open('ALM_voxels_symmetric.npy', 'rb')) # (n_voxels,3), xyz of ALM regions

        diff = ccf_coor[:,None,:] - ALM_voxels[None,:,:] # (n_neurons, n_voxels, 3)
        mask_region = (np.abs(diff[:,:,0]) <= res/2) * (diff[:,:,1] <= res/2) * (np.abs(diff[:,:,2]) <= res/2) # (n_neurons, n_voxels)
        mask_region = np.any(mask_region, axis=1) # (n_neurons,)

        neuron_id_region = neurons_all[mask_region]
    else:
        mask_region, neuron_id_region = get_id_subarea(region, ccf_label)
    '''

    # neuron_id_area = sorted(list( set(neuron_id_side) & set(neuron_id_region)))
    return neuron_id_area


def helper_filter_by_neuron_id(data_sess, neuron_id_area, probe_no, area_name):
    """
    Using the given neuron_id_area, filter the neuron sin data_sess.

    Arg:
        data_sess (dict): same format as sess_dict.
        neuron_id_area (list): list of neuron if to use.
        probe_no (int): probe number.
        area_name (str):  name of the area. e.g. 'left_ALM'.

    Returns:
        data_area (dict): same format as sess_dict.
    """
    data_area = {}

    data_area['mouse'] = data_sess['mouse']
    data_area['date'] = data_sess['date']
    data_area['probe'] = probe_no
    data_area['area'] = area_name

    data_area['bin_centers'] = data_sess['bin_centers']
    data_area['fr'] = data_sess['fr'][:,:,neuron_id_area] # (n_bins, n_trials, n_neurons)
    data_area['spike_times'] = data_sess['spike_times'][neuron_id_area] # ndarray. spike_times[neuron_id][trial_id] is ndarray with all spike times of this neuron in this trial. (go cue time is 0.)
    data_area['auto_learn_trials'] = data_sess['auto_learn_trials'] # ndarray, (n_trials,), int64
    data_area['early_lick_trials'] = data_sess['early_lick_trials'] # ndarray, (n_trials,), int64
    data_area['auto_water_trials'] = data_sess['auto_water_trials'] # ndarray, (n_trials,), int64
    data_area['free_water_trials'] = data_sess['free_water_trials'] # ndarray, (n_trials,), int64
    data_area['lick_directions'] = data_sess['lick_directions'] # ndarray, (n_trials). Each element is a list of 0 (lick right) and 1 (lick left)
    data_area['lick_times'] = data_sess['lick_times'] # list with length n_trials, each element is 1D ndarray with all lick times in that trial
    data_area['gocue_time'] = data_sess['gocue_time']
    data_area['correctness'] = data_sess['correctness'] # ndarray, (n_trials,), int64. 1: correct or free water, 0: error, -1: no response
    data_area['neuron_info'] = data_sess['neuron_info'][neuron_id_area,:] # ndarray (n_units, 8)

    data_area['ccf_coordinate'] = data_sess['ccf_coordinate'][neuron_id_area,:]  # ndarray (n_neurons,3), xyz coordinates
    data_area['ccf_label'] = list(np.array(data_sess['ccf_label'])[neuron_id_area]) # list of string, with whitespace at the beginning and end of each string removed
    data_area['ccf_unit_id'] = data_sess['ccf_unit_id'][neuron_id_area] # ndarray (n_neurons,), id of units with histology

    data_area['probe_info'] = 'reorganized'
    data_area['delay_period'] = data_sess['delay_period'] # (n_trials,) duration of delay period
    data_area['sample_period'] = data_sess['sample_period'] # (n_trials,) duration of sample period
    data_area['stimulation'] = data_sess['stimulation'] # ndarray, (n_trials,4). The 4 columns are [laser_power, stim_type('1', '2', or '6', with 1,2,6 being left/right/both ALM perturbation), laser_on_time, laser_off_time]. For no stim trials, we have [0, NaN, NaN, NaN]
    data_area['trial_type'] = data_sess['trial_type'] # ndarray, (n_trials,). 1 is left, 0 is right 'r'

    return data_area


def process_all_sess(bw, stride, begin_time, end_time, data_folder, save_folder, qc_mode):
    # scan through all sessions, and find files within the same session
    # all_path_list = sorted(list(glob.glob(os.path.join(data_folder, 'MAP_dandi', 'map*.mat')))) # SC065_20210507_172825_s8
    all_path_list = sorted(list(glob.glob(os.path.join(data_folder, 'MAP_dandi', 'map-export_SC050_20210302_140243_s20_p*.mat')))) # SC011_20190222_130111_s4

    name2paths = OrderedDict() # keys are part of file names that are the same for all sessions. Values are list of full file paths in that sess.
    for filepath in all_path_list:
        filename = os.path.split(filepath)[-1]
        name = filename[:-7]
        if name not in list(name2paths.keys()):
            name2paths[name] = []
        name2paths[name].append(filepath)

    for name, path_list in name2paths.items():
        print('Processing %s (%d files)' % (name, len(path_list)))
        process_one_sess(data_folder, path_list, bw, stride, begin_time, end_time, save_folder, qc_mode)


def process_all_sess_parallel(bw, stride, begin_time, end_time, data_folder, save_folder, qc_mode, n_cpu=multiprocessing.cpu_count()):
    # scan through all sessions, and find files within the same session
    all_path_list = sorted(list(glob.glob(os.path.join(data_folder, 'MAP_dandi', 'map*.mat')))) # SC065_20210507_172825_s8

    name2paths = OrderedDict() # keys are part of file names that are the same for all sessions. Values are list of full file paths in that sess.
    for filepath in all_path_list:
        filename = os.path.split(filepath)[-1]
        name = filename[:-7]
        if name not in list(name2paths.keys()):
            name2paths[name] = []
        name2paths[name].append(filepath)

    start_time = time.time()
    pool = multiprocessing.Pool(n_cpu)
    for name, path_list in name2paths.items():
        print('Processing %s (%d files)' % (name, len(path_list)))
        pool.apply_async(process_one_sess, args=(data_folder, path_list, bw, stride, begin_time, end_time, save_folder, qc_mode))
    pool.close()
    pool.join()
    print('The whole preprocessing took %.2f hours.' % ((time.time() - start_time)/3600))




if __name__ == "__main__":
    stride = 0.05
    bw = 0.1 # unit: sec
    begin_time = -3.0 # unit: sec
    end_time = 3.5
    qc_mode = 'classifier' # 'classifier' or 'old'

    # data_folder = os.path.join(os.pardir, 'data','data_without_CCF') # for local
    # data_folder = '/mnt/fs6/yiliu021/SC20200402/' # for sever
    # save_folder = os.path.join(os.pardir, 'data_processed','data_without_CCF')
    # save_folder = '/mnt/fs6/yiliu021/SC20200402_processed/' # for server

    data_folder = '/data2/MAP_data_2023Jul'
    save_folder = '/data2/MAP_data_processed_2023Jul'
    # save_folder_reorganize = '/data2/MAP_data_processed_final/data_with_CCF_2021May'

    # process_all_sess(bw, stride, begin_time, end_time, data_folder, save_folder, qc_mode)
    process_all_sess_parallel(bw, stride, begin_time, end_time, data_folder, save_folder, qc_mode, n_cpu=5)
