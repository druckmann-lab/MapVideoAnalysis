"""
Preprocessing utils from Yi for the raw .mat files of the MAP dataset
that were exported from DataJoint.

For the video analysis only the loadmat function is used, the rest is
only kept for completeness.
"""

import os, sys, time, pickle, argparse, math, glob, copy
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import numpy as np
import h5py
import scipy.io as spio
from sklearn.metrics import r2_score
from scipy import sparse, stats
from sklearn.model_selection import train_test_split


def loadmat(filename):
    '''
    this function should be called instead of direct spio.loadmat
    as it cures the problem of not properly recovering python dictionaries
    from mat files. It calls the function check ks to cure all entries
    which are still mat-objects
    '''
    def _check_ks(d):
        '''
        checks if entries in dictionary are mat-objects. If yes
        todict is called to change them to nested dictionaries
        '''
        for k in d:
            if isinstance(d[k], spio.matlab.mio5_params.mat_struct):
                d[k] = _todict(d[k])
        return d

    def _todict(matobj):
        '''
        A recursive function which constructs from matobjects nested dictionaries
        '''
        d = {}
        for strg in matobj._fieldnames:
            elem = matobj.__dict__[strg]
            if isinstance(elem, spio.matlab.mio5_params.mat_struct):
                d[strg] = _todict(elem)
            elif isinstance(elem, np.ndarray):
                d[strg] = _tolist(elem)
            else:
                d[strg] = elem
        return d

    def _tolist(ndarray):
        '''
        A recursive function which constructs lists from cellarrays
        (which are loaded as numpy ndarrays), recursing into the elements
        if they contain matobjects.
        '''
        elem_list = []
        for sub_elem in ndarray:
            if isinstance(sub_elem, spio.matlab.mio5_params.mat_struct):
                elem_list.append(_todict(sub_elem))
            elif isinstance(sub_elem, np.ndarray):
                elem_list.append(_tolist(sub_elem))
            else:
                elem_list.append(sub_elem)
        return elem_list
    data = spio.loadmat(filename, struct_as_record=False, squeeze_me=True)
    return _check_ks(data)


def loadh5mat(filename, avoid = [], source='dave'):
    '''
    source = 'dave' or 'susu'
    '''
    def _todict(h5obj):
        output = {}

        for k, v in h5obj.items():
            if k in avoid:
                continue
            elif isinstance(v, h5py._hl.files.File) \
                or isinstance(v, h5py._hl.group.Group):
                output[k] = _todict(v)
            elif isinstance(v, h5py._hl.dataset.Dataset):
                output[k] = np.array(v)
        return output

    datafile = h5py.File(filename)
    data = _todict(datafile)
    if source == 'dave': #Neural Data
        for k in ['clusterNotes', 'csNote_clu']:
            if k in data['S_clu'].keys():
                break
        for i, ref in enumerate(data['S_clu'][k][0]):
            # askii_list = datafile[ref].value.flatten() (Deprecated)
            askii_list = datafile[ref][()].flatten()
            data['S_clu'][k][0][i] = ''.join(map(chr, askii_list))
            if data['S_clu'][k][0][i]  == '\x00\x00':
                data['S_clu'][k][0][i] = 'noise'
    elif source == 'susu':
        if len(data['clusterNotes']) > 1: #[['note1'], ['note2'], ['note3']]
            for i in np.arange(len(data['clusterNotes'])):
                ref = data['clusterNotes'][i][0]
                askii_list = datafile[ref][()].flatten()
                data['clusterNotes'][i][0] = ''.join(map(chr, askii_list))
                if data['clusterNotes'][i][0]  == '\x00\x00':
                    data['clusterNotes'][i][0] = 'noise'
        elif len(data['clusterNotes'] == 1): #[['note1','note2','note3']]
            for i in np.arange(len(data['clusterNotes'][0])):
                ref = data['clusterNotes'][0][i]
                askii_list = datafile[ref][()].flatten()
                data['clusterNotes'][0][i] = ''.join(map(chr, askii_list))
                if data['clusterNotes'][0][i]  == '\x00\x00':
                    data['clusterNotes'][0][i] = 'noise'

    return data

"""
def random_split(input_array, test_ratio):
    '''
    randomly split the input_array into two parts
    @param input_array: 1D ndarray
    @param test_ratio: between 0 and 1
    returns 2 ndarray: output_array_1 (1-test_ratio), output_array_2 (test_ratio)
    '''
    # np.random.seed(1)
    n = len(input_array)
    idx = np.arange(n)
    n_test = int(np.ceil(n * test_ratio))
    np.random.shuffle(idx)
    idx_test = idx[:n_test]
    idx_train = idx[n_test:]

    output_test = input_array[idx_test]
    output_train = input_array[idx_train]
    return output_train, output_test
"""

def random_split(input_array, test_ratio):
    '''
    randomly split the input_array into two parts on the first axis
    @param input_array: multi-dimensional ndarray, with the first axis to be split
    @param test_ratio: between 0 and 1
    returns 2 ndarray: output_array_1 (1-test_ratio), output_array_2 (test_ratio)
    '''
    # np.random.seed(1)
    n = input_array.shape[0]
    idx = np.arange(n)
    n_test = int(np.ceil(n * test_ratio))
    np.random.shuffle(idx)
    idx_test = idx[:n_test]
    idx_train = idx[n_test:]

    output_test = input_array[idx_test,...]
    output_train = input_array[idx_train,...]
    return output_train, output_test


def shuffle_together(array_list):
    '''
    Shuffle all the ndarrays in array_list together along the first dimension. All arrays in array_list should have the same first dimension.
    '''
    dims = np.array([array.shape[0] for array in array_list])
    # print('dims:', dims)
    # output_list = []

    if not np.all(dims == dims[0]):
        print('Error: arrays do not agree on the first dimension!')
        return
    else:
        idx = np.arange(dims[0])
        np.random.shuffle(idx) # shuffle the index
        output_list = [array[idx,...] for array in array_list]

    return output_list


def random_split_keep_ratio(x, y, test_ratio=0.2):
    """
    Randomly splitting input x according to the first dimension, keeping the ratio of different categories (binary label y) the same, and then shuffle samples.

    Args:
        x (ndarray): (n_samples, ...) input data
        y (ndarray): (n_samples, ) binary labels, 0 or 1.

    Returns:
        x_train (ndarray): (n_samples_train, ...)
        y_train (ndarray): (n_samples_train, )
        x_test (ndarray): (n_samples_test, ...)
        y_test (ndarray): (n_samples_test, )
    """
    assert x.shape[0] == len(y)

    x0 = x[y==0,...]
    y0 = y[y==0]
    x1 = x[y==1,...]
    y1 = y[y==1]

    x0_train, x0_test, y0_train, y0_test = train_test_split(x0, y0, test_size=test_ratio)
    x1_train, x1_test, y1_train, y1_test = train_test_split(x1, y1, test_size=test_ratio)

    x_train = np.concatenate([x0_train, x1_train])
    y_train = np.concatenate([y0_train, y1_train])
    x_test = np.concatenate([x0_test, x1_test])
    y_test = np.concatenate([y0_test, y1_test])

    x_train, y_train = shuffle_together([x_train, y_train])
    x_test, y_test = shuffle_together([x_test, y_test])

    return x_train, y_train, x_test, y_test


def normalize(x, axis):
    '''
    normalize x along axis(1 or 0)
    x: 2D ndarray, with shape (n_samples, n_features) or (n_features, n_samples)
    '''
    return x / np.sqrt(np.sum(x**2, axis=axis, keepdims=True))


def my_r2_score(y_true, y_pred):
    numerator = np.sum((y_pred - y_true)**2, axis=0)
    denominator = np.sum((y_true - np.average(y_true, axis=0))**2, axis=0)
    nonzero_denominator = denominator != 0
    nonzero_numerator = numerator != 0
    valid_score = nonzero_denominator & nonzero_numerator

    output_scores = 1 - numerator[valid_score] / denominator[valid_score]
    if 0 in denominator:
        print('Altogether:', denominator.shape)
        print('The number of 0:', np.where(denominator==0))
    # print(numerator.shape)
    # print(denominator.shape)
    return np.average(output_scores, axis=0)


def check_fr(fr):
    '''
    check if there are bad (i.e. zero-variance) recordings in fr. If so, delete the corresponding neuron.
    @param fr: firing rates to be checked, (n_bins, n_trials, n_clusters)
    @return fr_checked：(n_bins, n_trials, n_good_clusters)
    @return idx_array: 1D array of int, the indices of bad neurons.
    '''

    var_fr = np.var(fr, axis=1) # variance across trials. (n_bins, n_clusters)
    zero_dims = np.where(var_fr==0)
    # print(zero_dims)
    bad_cluster = np.unique(zero_dims[1])
    fr_good = np.delete(fr, obj=bad_cluster, axis=2)
    return fr_good, bad_cluster


def check_cov(X_tp, label):
    '''
    check if the covariance matrix of X_tp is singular (i.e. determinant=0)
    @param X_tp: (n_trials, n_clusters)
    @param label: sring, labelling what variable this is
    '''
    cov_X = np.cov(X_tp, rowvar=False)
    # if np.linalg.det(cov_X) == 0:
    # print('------------------')
    print(label, 'shape:', cov_X.shape)
    print(np.linalg.det(cov_X))


# set the colormap and centre the colorbar
class MidpointNormalize(colors.Normalize):
    """Normalise the colorbar so that diverging bars work there way either side from a prescribed midpoint value)

    e.g. im=ax1.imshow(array, cmap=matplotlib.cm.RdBu_r, norm=MidpointNormalize(midpoint=0.,vmin=-100, vmax=100))
    """
    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):
        self.midpoint = midpoint
        colors.Normalize.__init__(self, vmin, vmax, clip)

    def __call__(self, value, clip=None):
        # I'm ignoring masked values and all kinds of edge cases to make a
        # simple example...
        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]
        return np.ma.masked_array(np.interp(value, x, y), np.isnan(value))


def get_p_value(x1, x2, one_tailed=True):
    """
    Calculate the p value (one-tailed) between two groups.

    Args:
        x1 (ndarray): (..., n1_smaples)
        x2 (ndarray): (...(same as x1), n2_smaples)
        one_tailed (bool): whether to perform one-tailed or two_tailed t test. Both have the same t-score, but p_value(2-tailed) = 2*p_value(1-tailed).

    Returns:
        t_score (adarray or scaler): dimension same as the '...' of x1 or x2.
        pval (adarray or scaler): dimension same as t_score
    """
    n1 = x1.shape[-1]
    n2 = x2.shape[-1]
    df = n1 +n2 - 2 # degree of freedom

    Sp = ( (n1-1)*np.var(x1,axis=-1) + (n2-1)*np.var(x2,axis=-1) ) / df
    s = np.sqrt(Sp*(1/n1 + 1/n2))

    t_score = (np.mean(x2, axis=-1) - np.mean(x1, axis=-1)) / s # （n_bins,n_neurons）
    # if isinstance(a, np.array): # not scalar
    #     t_score[np.isnan(t_score)] = 0 # sometimes Sp=0 (since var=0). This will result in nan in d_prime, so here nan is turned into 0.

    pval = stats.t.sf(np.abs(t_score), n1+n2-2) # one-tailed

    if not one_tailed:
        pval = 2*pval

    return t_score, pval


def get_period(period):
    """
    Args:
        period (str): 'all', 'sample', 'delay', or 'post_go'.
    """
    if period == 'all':
        t_start, t_end = -3.0, 3.5
    elif period == 'sample':
        t_start, t_end = -1.9, -1.2
    elif period == 'delay':
        t_start, t_end = -1.2, 0.0
    elif period == 'post_go':
        t_start, t_end = 0.0, 1.0
    return t_start, t_end
